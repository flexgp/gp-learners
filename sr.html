<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>SR learner</title>
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script src="javascripts/main.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

  </head>
  <body>

      <header>
        <h1><a href="index.html">GP Learners</a></h1>
        <p>SR learner</p>
      </header>

    <div class="wrapper">
      <nav>
        <ul></ul>
      </nav>
      <section>
        <h1>
<a name="flexgp-machine-learning-with-genetic-programming" class="anchor" 
href="#flexgp-machine-learning-with-genetic-programming"><span class="octicon 
octicon-link"></span></a>SR learner</h1>

<p>Current release provides functionality both for performing Symbolic Regression on numerical datasets and 
for testing the retrieved models. In this page we provide a quick tutorial on how to get started with SRLearner.</p>

<h1>
<a name="tutorial" class="anchor" href="#tutorial"><span class="octicon octicon-link"></span></a>Tutorial</h1>

<p>Note: this release is only supported for Linux Debian platforms.</p>

<h2>
<a name="step-2-data-format" class="anchor" href="#step-2-data-format"><span class="octicon 
octicon-link"></span></a>Step 1: Data format</h2>

<p>Data must be provided in csv format where each line corresponds to an exemplar and the target values are placed 
in the last column. Note that any additional line or column containing labels or nominal values needs to be removed.</p>

<h2><a name="step-1-download-the-flexgpjar-file-from-here" class="anchor" 
href="#step-1-download-the-flexgpjar-file-from-here"><span class="octicon 
octicon-link"></span></a>Step 2: Download the sr.jar file from <a href="downloads/sr.jar">here</a></h2>

<h2><a name="step-3-running-flexgp" class="anchor" href="#step-3-running-flexgp"><span class="octicon 
octicon-link"></span></a>Step 3 (optional): Normalize the data</h2>

<p>The SR learner provides functionality to normalize the data. The path to the normalized data and bounds for the
 explanatory variables and targets need to be specified:</p>
<pre><code>$ java -jar sr.jar -normalizeData path_to_data -newPath path_to_normalized_data -pathToBounds path_to_variable_bounds
</code></pre>


<h2><a name="step-3-running-flexgp" class="anchor" href="#step-3-running-flexgp"><span class="octicon 
octicon-link"></span></a>Step 4: Running SR learner</h2>

<p>In the current release, it is only possible to run SR learner directly from your terminal (a Matlab wrapper 
will be included soon).</p>

<h3><a name="running-flexgp-from-the-terminal" class="anchor" href="#running-flexgp-from-the-terminal"><span 
class="octicon octicon-link"></span></a>Running SR learner from the terminal</h3>

<h4>
<a name="model-the-data" class="anchor" href="#model-the-data"><span class="octicon octicon-link"></span></a>Model 
the data</h4>

<p>All you need to provide is the path to your dataset (whether normalized or original) and the optimization time</p>

<pre><code>$ java -jar sr.jar -train path_to_your_data -minutes 10 
</code></pre>

<p>At the end of the run a set of files are generated:</p>

<ol>
<li><p><strong>pareto.txt</strong>: models forming the Pareto Front (accuracy vs model complexity).</p></li>
<li><p><strong>leastComplex.txt</strong>: least complex model of the Pareto Front.</p></li>
<li><p><strong>mostAccurate.txt</strong>: most accurate model of the Pareto Front.</p></li>
<li><p><strong>knee.txt</strong>: model at the knee of the Pareto Front.</p></li>
<li><p><strong>bestModelGeneration.txt</strong>: most accurate model per generation.</p></li>
<li><p><strong>fusedModel.txt</strong>: fused model of the Pareto Front obtained with Adaptive Regression 
by Mixing (see <em>Adaptive Regression by Mixing. Yuhong Yang. Journal of the American Statistical Association 
Vol. 96, No. 454 (Jun., 2001), pp. 574-588</em>).</p></li>
</ol>

<h4><a name="test-the-models" class="anchor" href="#test-the-models"><span class="octicon octicon-link"></span></a>Test 
the models</h4>

<p>The SR learner provides functionality to obtain the Mean Squared Error (MSE) and Mean Average Error (MAE) of 
 the retrieved models once the training is finished. To automatically test all the generated models, type:</p>

<pre><code>$ cd run_folder
$ java -jar sr.jar -test path_to_your_data
</code></pre>

<p>If the data has been normalized, it is necessary to specify the path to the stored min and max bounds of both the
training and test sets to obtain predictions in the original scale:</p>

<pre><code>$ cd run_folder
$ java -jar sr.jar -normTest path_to_norm_test_data -pathToTrainBounds path_to_tr_bounds -pathToTestBounds path_to_test_bounds
</code></pre>

<h3>
<a name="running-flexgp-from-matlab" class="anchor" href="#running-flexgp-from-matlab"><span
class="octicon octicon-link"></span></a>Running SR learner from Matlab</h3>

<p>To be done</p>


<h1><a name="bellsandwhistles" class="anchor" href="#tutorial"><span class="octicon 
octicon-link"></span></a>Bells and whistles</h1>

<h2>
<a name="step-3-speeding-up-your-runs-with-c-optimized-execution" class="anchor" 
href="#step-3-speeding-up-your-runs-with-c-optimized-execution"><span class="octicon 
octicon-link"></span></a>1) Speeding up your runs with C++ optimized execution</h2>

<p>This option requires the installation of the gcc and g++ compilers and the configuration of the Linux kernel parameter governing the maximum size of shared memory segments:</p>

<pre><code>$ sudo apt-get install gcc
$ sudo apt-get install g++
</code></pre>

<p>Modify the Linux kernel parameter governing the maximum shared memory segment size to be at least as large as the data being analyzed, in the next example we set it to 2GB</p>

<pre><code>$ sudo echo 2147483648 &gt; /proc/sys/kernel/shmmax
</code></pre>

<p>To benefit from the optimized C++ execution, append the -cpp flag followed by the number of CPU threads that will be employed to speedup FlexGP (4 in the example below). Additionally, it is necessary to create an auxiliary default folder in which temporary C++ files will be generated:</p>

<pre><code>$ mkdir tempFiles
$ java -jar sr.jar -train path_to_your_data -minutes 10 -cpp 4
</code></pre>

<h2>
<a name="step-3-speeding-up-your-runs-with-c-optimized-execution" class="anchor" 
href="#step-3-speeding-up-your-runs-with-c-optimized-execution"><span class="octicon 
octicon-link"></span></a>2) Speeding up your runs with CUDA</h2>

<p>This option requires the installation of the gcc, g++, and nvcc compilers and the configuration of the Linux kernel parameter 
governing the maximum size of shared memory segments. To benefit from the CUDA execution, append the <em>-cuda</em> flag. Additionally, it is necessary to create an auxiliary default folder in which temporary CUDA (.cu) files will be generated:</p>

<pre><code>$ mkdir tempFiles
$ java -jar sr.jar -train path_to_your_data -minutes 10 -cuda
</code></pre>


<h2>
<a name="step-4-speeding-up-your-runs-with-c-optimized-execution" class="anchor" 
href="#step-4-speeding-up-your-runs-with-c-optimized-execution"><span class="octicon 
octicon-link"></span></a>3) Change the default parameters</h2>

<p>To modify the default parameters of the SR learner, it is necessary to append the flag <em>-properties</em>
 followed by the path of the properties file containing the desired parameters:
 
<pre><code>$ java -jar sr.jar -train path_to_your_data -minutes 10 -cpp 4 -properties path_to_props_file
</code></pre>
 
The following properties file example specifies the population size, the features that will be
 considered during the learning process, the functions employed to generate GP trees, the tournament selection size,
 and the mutation rate.
 
  </p>
<pre><code>pop_size = 1000
terminal_set = X1 X3 X7 X8 X9 X12 X13 X17
function_set = + - * mydivide exp mylog sqrt square cube quart sin cos
tourney_size = 10
mutation_rate = 0.1
</code></pre>

<h1>
<a name="examples" class="anchor" href="#examples"><span class="octicon octicon-link"></span></a>Examples</h1>

<p>To check reports visit our blog:
<a href="blog.html">FlexGP Blog</a></p>

<h1>
<a name="authors-and-contributors" class="anchor" href="#authors-and-contributors"><span class="octicon octicon-link"></span></a>Authors and Contributors</h1>

<p><a href="http://flexgp.csail.mit.edu/">FlexGP</a> is a project of the <a href="http://groups.csail.mit.edu/EVO-DesignOpt/groupWebSite/">Any-Scale Learning For All (ALFA)</a> group at MIT.
<center><img src="images/ALFA-logo-lousy.png" alt="ALFA" align="middle" Vspace="45" height="100px"></center></p>


      </section>
      <footer>
        <p><small>Hosted on <a href="http://pages.github.com">GitHub Pages</a> using the Dinky theme</small></p>
      </footer>
    </div>	
  </body>
</html>
