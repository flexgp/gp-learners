<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>GP Function Classification</title>
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script src="javascripts/main.js"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

  </head>
  <body>

      <header>
        <h1><a href="index.html">FlexGP Project</a></h1>
        <p>GP Function learner</p>
      </header>

    <div class="wrapper">
      <nav>
        <ul></ul>
      </nav>
      <section>
        <h1>
<a name="flexgp-machine-learning-with-genetic-programming" class="anchor" 
href="#flexgp-machine-learning-with-genetic-programming"><span class="octicon 
octicon-link"></span></a>GP Function classifier</h1>

<p>This novel classification approach evolves GP functions to tackle binary classification problems. A decision
 threshold is employed such that a prediction is positive if the outcome of the GP function is greater than the
 threshold and negative otherwise.</p>
 
<p>The training process is divided into two steps. First, a set of classifiers are evolved via Genetic Programming. 
 In a second step, the decision threshold is computed for each classifier. It is possible
 to use disjoint sets of data (training and validation sets) for the first and seconds steps. For further
 details of the GP Function classifier, the reader is referred to
 <a href="http://biglearn.org/2013/files/papers/biglearning2013_submission_21.pdf"> this paper</a>:</p>
 <p><em>Arnaldo, I.; Veeramachaneni, K; O'Reilly, UM: Building Multiclass Nonlinear Classifiers with GPUs.
 Big Learning Workshop at NIPS: Advances in Algorithms and Data Management, 2013.</em></p>
<p>and:</p>
 <p><em>Ignacio Arnaldo, Kalyan Veeramachaneni, Andrew Song, Una-May Oâ€™Reilly: Bring Your Own Learner! A cloud-based, data-parallel commons for machine learning.
To appear in IEEE Computational Intelligence Magazine. Special Issue on Computational Intelligence for Cloud Computing (Feb. 2015).</em></p>

<h1><a name="tutorial" class="anchor" href="#tutorial"><span class="octicon octicon-link"></span></a>Tutorial</h1>

<p>Current release provides functionality both for performing Binary Classification on numerical datasets and 
for testing the retrieved classifiers. In this page we provide a quick tutorial on how to get started with the
 GP Function classifier.</p>
<p>Note: this release is only supported for Linux Debian platforms.</p>

<h2><a name="step-2-data-format" class="anchor" href="#step-2-data-format"><span class="octicon 
octicon-link"></span></a>Step 1: Data format</h2>

<p>Data must be provided in csv format where each line corresponds to an exemplar and the target values are placed 
in the last column. Note that any additional line or column containing labels or nominal values needs to be removed.</p>

<h2><a name="step-1-download-the-flexgpjar-file-from-here" class="anchor" 
href="#step-1-download-the-flexgpjar-file-from-here"><span class="octicon 
octicon-link"></span></a>Step 2: Download the gpfunction.jar file from <a href="downloads/gpfunction.jar" target="_blank">here</a></a>
</h2>

<h2>
<a name="step-3-speeding-up-your-runs-with-c-optimized-execution" class="anchor" 
href="#step-3-speeding-up-your-runs-with-c-optimized-execution"><span class="octicon 
octicon-link"></span></a>Step 3: Configuring the C++ environment</h2>

<p>This learner requires the installation of the gcc and g++ compilers and the configuration of the Linux kernel 
parameter governing the maximum size of shared memory segments:</p>

<pre><code>$ sudo apt-get install gcc
$ sudo apt-get install g++
</code></pre>

<p>Modify the Linux kernel parameter governing the maximum shared memory segment size to be at least as large as the
data being analyzed, in the next example we set it to 2GB</p>

<pre><code>$ sudo echo 2147483648 &gt; /proc/sys/kernel/shmmax
</code></pre>

or modify it manually:
<pre><code>$ sudo nano /proc/sys/kernel/shmmax
</code></pre>


<h2>
<a name="step-3-running-flexgp" class="anchor" href="#step-3-running-flexgp"><span class="octicon 
octicon-link"></span></a>Step 4: Training GP Function classifiers</h2>

<p>In the current release, it is only possible to train the GP Function classifier directly from your terminal.</p>

<h3>
<a name="running-flexgp-from-the-terminal" class="anchor" href="#running-flexgp-from-the-terminal"><span 
class="octicon octicon-link"></span></a>Running GP Function classifier from the terminal</h3>

<h4>
<a name="model-the-data" class="anchor" href="#model-the-data"><span class="octicon octicon-link"></span></a>Model 
the data</h4>

<p>First, it is necessary to create an auxiliary folder in which temporary C++ files will be
 generated. The parameters required to train the classifier are the path to your dataset, the path to the validation
 set, and the optimization time:</p>

<pre><code>$ mkdir tempFiles
$ java -jar gpfunction.jar -train path_to_train_data -cv path_to_validation_data -minutes 10
</code></pre>

<p>At the end of the run a set of files are generated:</p>

<ol>
<li><p><strong>bestCrossValidation.txt</strong>: model with higest area under the ROC curve when evaluated on the validation set.</p></li>
<li><p><strong>pareto.txt</strong>: models forming the Pareto Front (accuracy vs model complexity).</p></li>
<li><p><strong>leastComplex.txt</strong>: least complex model of the Pareto Front.</p></li>
<li><p><strong>mostAccurate.txt</strong>: most accurate model of the Pareto Front.</p></li>
<li><p><strong>knee.txt</strong>: model at the knee of the Pareto Front.</p></li>
<li><p><strong>bestModelGeneration.txt</strong>: most accurate model per generation.</p></li>
</ol>


<h4><a name="test-the-models" class="anchor" href="#test-the-models"><span class="octicon octicon-link"></span></a>Test 
the models</h4>

<p>The GP Function learner provides functionality to obtain the accuracy, precision, recall, F-score, false positive rate,
and false negative rate of the retrieved classfiers once the training is finished. To automatically test all the
generated classifiers, type:</p>

<pre><code>$ cd run_folder
$ java -jar gpfunction.jar -test path_to_test_data
</code></pre>


<h1>
<a name="bellsandwhistles" class="anchor" href="#tutorial"><span class="octicon 
octicon-link"></span></a>Bells and whistles</h1>


<h2>
<a name="step-4-speeding-up-your-runs-with-c-optimized-execution" class="anchor" 
href="#step-4-speeding-up-your-runs-with-c-optimized-execution"><span class="octicon 
octicon-link"></span></a>1) Specify the number of CPU threads</h2>

<p>To speedup the training process, append the <em>-cpp</em> flag followed by the number of CPU threads that will be employed 
 to evaluate the candidate solutions (4 in the example below):
 
<pre><code>$ java -jar gpfunction.jar -train path_to_train_data -cv path_to_cv_data -minutes 10 -cpp 4
</code></pre>

<h2>
<a name="step-4-speeding-up-your-runs-with-c-optimized-execution" class="anchor" 
href="#step-4-speeding-up-your-runs-with-c-optimized-execution"><span class="octicon 
octicon-link"></span></a>2) Change the default parameters</h2>

<p>To modify the default parameters of the GP Function learner, it is necessary to append the flag 
 <em>-properties</em> followed by the path of the properties file containing the desired parameters:
 
<pre><code>$ java -jar gpfunction.jar -train path_to_train_data -cv path_to_cv_data -minutes 10 -cpp 4 -properties path_to_props_file
</code></pre>
 
The following properties file example specifies the population size, the features that will be
 considered during the learning process, the functions employed to generate GP trees, the tournament selection size,
 and the mutation rate.
 
<pre><code>pop_size = 1000
terminal_set = X1 X3 X7 X8 X9 X12 X13 X17
function_set = + - * mydivide exp mylog sqrt square cube quart sin cos
tourney_size = 10
mutation_rate = 0.1
</code></pre>

<h1>
<a name="examples" class="anchor" href="#examples"><span class="octicon octicon-link"></span></a>Examples</h1>

<p>To check reports visit our blog:
<a href="blog.html">FlexGP Blog</a></p>

<h1>
<a name="authors-and-contributors" class="anchor" href="#authors-and-contributors"><span class="octicon octicon-link"></span></a>Authors and Contributors</h1>

<p>This project is maintained by the <a href="http://groups.csail.mit.edu/EVO-DesignOpt/groupWebSite/">Any-Scale Learning For All (ALFA)</a> group at MIT.
<img src="images/ALFA-logo-lousy.png" alt="ALFA"></p>
      </section>
      <footer>
        <p><small>Hosted on <a href="http://pages.github.com">GitHub Pages</a> using the Dinky theme</small></p>
      </footer>
    </div>	
  </body>
</html>
